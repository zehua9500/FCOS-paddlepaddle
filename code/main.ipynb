{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!unzip data/data11713/train1_20190818.zip -d work/train_dat/\n",
    "#!unzip data/data11713/guangdong1_round1_testA_20190818.zip -d work/test_data/\n",
    "#!zip -r work/train_data.zip work/train_dat/guangdong1_round1_train1_20190818\n",
    "#!unzip data/data7122/test2017.zip -d work/coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(4).reshape(2,2)\n",
    "b = np.array([[True,True],[False,True]])\n",
    "c = a[b]\n",
    "d = np.array([1,2,3])\n",
    "np.hstack([c,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import paddle.fluid as fluid\n",
    "import paddle \n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "with fluid.dygraph.guard():\n",
    "    a = np.array([1,2,3,45])\n",
    "    a = fluid.dygraph.to_variable(a)\n",
    "    print(a>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import paddle.fluid as fluid\n",
    "import paddle \n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(r\"work/fcos\")\n",
    "from FCOS import FCOS\n",
    "\n",
    "\"\"\"\n",
    "from coco_dataloader import Coco_datGenerator\n",
    "from coco_valdataloader import val\n",
    "batch_size = 4\n",
    "train_generator = Coco_datGenerator(batchsize = batch_size)\n",
    "train_reader = train_generator.generator\n",
    "img = fluid.layers.data(name='img', dtype='float32', shape=[3,640,640])\n",
    "cls = fluid.layers.data(name='cls', dtype='float32', shape=[8525, 90])\n",
    "cen = fluid.layers.data(name='cen', dtype='float32', shape=[8525])\n",
    "reg = fluid.layers.data(name='reg', dtype='float32', shape=[8525, 4])\n",
    "cen_mask = fluid.layers.data(name='cen_mask', dtype='float32', shape=[8525])\n",
    "py_reader = fluid.io.PyReader(feed_list=[img, cls, reg, cen, cen_mask], capacity=2, iterable=True)\n",
    "py_reader.decorate_batch_generator(train_reader, places=fluid.cuda_places(0))\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 1\n",
    "from data import datGenerator\n",
    "train_generator = datGenerator()\n",
    "train_reader = train_generator.generator\n",
    "img = fluid.layers.data(name='img', dtype='float32', shape=[3,1000,2446])\n",
    "cls = fluid.layers.data(name='cls', dtype='float32', shape=[51137, 20])\n",
    "cen = fluid.layers.data(name='cen', dtype='float32', shape=[51137])\n",
    "reg = fluid.layers.data(name='reg', dtype='float32', shape=[51137, 4])\n",
    "cen_mask = fluid.layers.data(name='cen_mask', dtype='float32', shape=[51137])\n",
    "\n",
    "py_reader = fluid.io.PyReader(feed_list=[img, cls, reg, cen, cen_mask], capacity=2, iterable=True)\n",
    "py_reader.decorate_batch_generator(train_reader, places=fluid.cuda_places(0))\n",
    "#py_reader.decorate_batch_generator(train_reader, places=fluid.cpu_places(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-45f144663648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfluid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdygraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "with fluid.dygraph.guard():\n",
    "    print(data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCOS load final\n",
      "No optimizer loaded. If you didn't save optimizer, please ignore this. The program can still work with new optimizer. \n",
      "training begin.....\n",
      "img nums =  12872\n",
      "epoch = 10 | iter = 0 | loss = 0.07164 | focal_loss = 0.04504 | iou_loss = 0.02347 | centerness_loss = 0.00312 | use time = 2.225 s | time is 16:56:06 2019\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\r\n",
    "learning_rate = 5e-6\r\n",
    "start = time.time()\r\n",
    "\r\n",
    "#cls_loss_hist = collections.deque(maxlen=300)\r\n",
    "#reg_loss_hist = collections.deque(maxlen=300)\r\n",
    "#cen_loss_hist = collections.deque(maxlen=300)\r\n",
    "cls_loss_hist = []\r\n",
    "reg_loss_hist = []\r\n",
    "cen_loss_hist = []\r\n",
    "dice_loss_hist = []\r\n",
    "learning_rate = fluid.layers.cosine_decay( learning_rate = learning_rate, step_each_epoch=12872//10, epochs=60)\r\n",
    "adam = fluid.optimizer.AdamOptimizer(learning_rate = learning_rate)\r\n",
    "\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    model = FCOS(\"fcos\", 20, batch=batch_size)\r\n",
    "    parameters, _ = fluid.dygraph.load_persistables(r\"work/model/v4/7head_eps = 10_half\")\r\n",
    "    model.load_dict(parameters)\r\n",
    "    print(\"training begin.....\")\r\n",
    "    for epoch in range(10, epochs+10):\r\n",
    "        for idx, data in enumerate(py_reader()):\r\n",
    "            #if(data[2].shape[0] > batch_size):\r\n",
    "            #    print(\"batch error  \")\r\n",
    "            #    break\r\n",
    "            data[4].stop_gradient = True\r\n",
    "            data[1].stop_gradient = True\r\n",
    "            data[2].stop_gradient = True\r\n",
    "            data[3].stop_gradient = True\r\n",
    "            data[5].stop_gradient = True\r\n",
    "            data[6].stop_gradient = True\r\n",
    "            data[7].stop_gradient = True\r\n",
    "            f_loss, i_loss, c_loss = model(data[0], data[1], data[2], data[3],data[4],data[5],data[6],data[7])\r\n",
    "            loss = f_loss + i_loss + c_loss# + d_loss * fluid.layers.reduce_sum(data[4]) * 0.1 / batch_size\r\n",
    "            loss.backward()\r\n",
    "            adam.minimize(loss)\r\n",
    "            cls_loss_hist.append(f_loss.numpy())\r\n",
    "            reg_loss_hist.append(i_loss.numpy())\r\n",
    "            cen_loss_hist.append(c_loss.numpy())\r\n",
    "            #dice_loss_hist.append(d_loss.numpy())\r\n",
    "            if(idx % 300 == 0):\r\n",
    "                cls_loss_mean = np.mean(cls_loss_hist)\r\n",
    "                reg_loss_mean = np.mean(reg_loss_hist)\r\n",
    "                cen_loss_mean = np.mean(cen_loss_hist)\r\n",
    "                #dice_loss_mean = np.mean(dice_loss_hist)\r\n",
    "                cls_loss_hist = []\r\n",
    "                reg_loss_hist = []\r\n",
    "                cen_loss_hist = []\r\n",
    "                #dice_loss_hist = []\r\n",
    "                loss_mean = cls_loss_mean + reg_loss_mean + cen_loss_mean# + dice_loss_mean\r\n",
    "                #print(time.asctime( time.localtime(time.time()) )[11:])\r\n",
    "                print(\"epoch = %d | iter = %d | loss = %.5f | focal_loss = %.5f | iou_loss = %.5f | centerness_loss = %.5f | use time = %.3f s | time is %s\"%\\\r\n",
    "                (epoch, idx, loss_mean, cls_loss_mean, reg_loss_mean, cen_loss_mean, time.time() - start, time.asctime(time.localtime(time.time()) )[11:]))\r\n",
    "                start = time.time()\r\n",
    "            model.clear_gradients()\r\n",
    "        fluid.dygraph.save_persistables(model.state_dict(), \"work/model/v4/7head_eps = %s\"%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fluid.dygraph.save_persistables(model.state_dict(), \"work/model/v4/7head_eps = 10_half\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Bottleneck(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, planes, is_test, stride = 1, downsample=None):\n",
    "        super(Bottleneck, self).__init__(name_scope)\n",
    "        self.conv1 = fluid.dygraph.Conv2D(\"conv1\", planes, 1)\n",
    "        self.bn1 = fluid.dygraph.BatchNorm(\"bn1\",planes, act = \"relu\", is_test = is_test)\n",
    "        \n",
    "        self.conv2 = fluid.dygraph.Conv2D(\"conv2\", planes, 3, padding = 1, stride=stride)\n",
    "        self.bn2 = fluid.dygraph.BatchNorm(\"bn2\",planes, act = \"relu\", is_test = is_test)\n",
    "        \n",
    "        self.conv3 = fluid.dygraph.Conv2D(\"conv3\", planes * 4, 1)\n",
    "        self.bn3 = fluid.dygraph.BatchNorm(\"bn3\",planes * 4, act = \"relu\", is_test = is_test)\n",
    "        \n",
    "        #self.relu = fluid.layers.relu\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "        x = x + residual\n",
    "        return self.bn3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Make_layer(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope, planes, layernums, is_test, stride = 1):\r\n",
    "        super(Make_layer, self).__init__(name_scope)\r\n",
    "        self.layernums = layernums\r\n",
    "        \r\n",
    "        self.downsample = fluid.dygraph.Conv2D(\"downsample\", planes * 4, 1, stride = stride)\r\n",
    "        self.layer1 = Bottleneck(\"layer1\", planes, stride = stride, is_test = is_test, downsample = self.downsample)\r\n",
    "        \r\n",
    "        self.layer2 = Bottleneck(\"layer2\", planes, is_test = is_test)\r\n",
    "        self.layer3 = Bottleneck(\"layer3\", planes, is_test = is_test)\r\n",
    "        \r\n",
    "        if(layernums >= 4):\r\n",
    "            self.layer4 = Bottleneck(\"layer4\", planes, is_test = is_test)\r\n",
    "            \r\n",
    "        if(layernums >= 6):\r\n",
    "            self.layer5 = Bottleneck(\"layer5\", planes, is_test = is_test)\r\n",
    "            self.layer6 = Bottleneck(\"layer6\", planes, is_test = is_test)\r\n",
    "            \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.layer1(x)\r\n",
    "        x = self.layer2(x)\r\n",
    "        x = self.layer3(x)\r\n",
    "        if(self.layernums >= 4):\r\n",
    "            x = self.layer4(x)\r\n",
    "            \r\n",
    "        if(self.layernums >= 6):\r\n",
    "            x = self.layer5(x)\r\n",
    "            x = self.layer6(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SEConcat(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, channel = 256):\n",
    "        super(SEConcat, self).__init__(name_scope)\n",
    "        self.downChannel = fluid.dygraph.Conv2D(\"downChannel\", 256,filter_size = 1, stride=1)\n",
    "        self.fc = fluid.dygraph.FC(\"fc\", size = 256, act=\"sigmoid\", is_test=False, dtype='float32')\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = fluid.layers.concat(input=[x1, x2], axis=1)\n",
    "        x = self.downChannel(x)\n",
    "        mean = fluid.layers.reduce_mean(x,dim = [2,3])\n",
    "        mean = self.fc(mean)\n",
    "        return fluid.layers.elementwise_mul(x, mean, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FPN(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, is_test, channel = 256):\n",
    "        super(FPN, self).__init__(name_scope)\n",
    "        self.P5_1 = fluid.dygraph.Conv2D(\"P5_1\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.P5_2 = fluid.dygraph.Conv2D(\"P5_2\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.P5_up = fluid.dygraph.Conv2D(\"P5_up\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.se5to4 = SEConcat(\"se5to4\")\n",
    "        \n",
    "        self.P4_1 = fluid.dygraph.Conv2D(\"P4_1\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.P4_2 = fluid.dygraph.Conv2D(\"P4_2\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.P4_up = fluid.dygraph.Conv2D(\"P4_up\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.se4to3 = SEConcat(\"se4to3\")\n",
    "        \n",
    "        self.P3_1 = fluid.dygraph.Conv2D(\"P3_1\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        self.P3_2 = fluid.dygraph.Conv2D(\"P3_2\", channel, filter_size = 3, stride=1, padding=1)\n",
    "        \n",
    "        self.P6_1 = fluid.dygraph.Conv2D(\"P6_1\", channel, filter_size = 3, stride=2, padding=1)\n",
    "        self.bn6 = fluid.dygraph.BatchNorm(\"bn6\",channel, act = \"relu\", is_test = is_test)\n",
    "        \n",
    "        self.P7_1 = fluid.dygraph.Conv2D(\"P7_1\", channel, filter_size = 3, stride=2, padding=1)\n",
    "    def forward(self, inputs):\n",
    "        C3,C4,C5 = inputs\n",
    "        P5_x = self.P5_1(C5)\n",
    "        P5_upsample = fluid.layers.resize_nearest(input = P5_x, scale=None, out_shape = (63, 153))\n",
    "        P5_upsample = self.P5_up(P5_upsample)\n",
    "        P5_x = self.P5_2(P5_x)\n",
    "        \n",
    "        P4_x = self.P4_1(C4)\n",
    "        #P4_x = P4_x + P5_upsample\n",
    "        P4_x = self.se5to4(P4_x, P5_upsample)\n",
    "        P4_upsample = fluid.layers.resize_nearest(input = P4_x, scale=None, out_shape = (125, 306))\n",
    "        P4_upsample = self.P4_up(P4_upsample)\n",
    "        P4_x = self.P4_2(P4_x)\n",
    "        \n",
    "        P3_x = self.P3_1(C3)\n",
    "        #P3_x = P3_x + P4_upsample\n",
    "        P3_x = self.se4to3(P3_x, P4_upsample)\n",
    "        P3_x = self.P3_2(P3_x)\n",
    "        \n",
    "        P6_x = self.P6_1(C5)\n",
    "        P6_bn = self.bn6(P6_x)\n",
    "        \n",
    "        P7_x = self.P7_1(P6_bn)\n",
    "        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ASPP(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope, is_test):\r\n",
    "        super(ASPP, self).__init__(name_scope)\r\n",
    "        self.dilate1 = fluid.dygraph.Conv2D(\"dilate1\", num_filters = 64, filter_size = 3, stride=1, padding=1, dilation=1)\r\n",
    "        self.dilate2 = fluid.dygraph.Conv2D(\"dilate1\", num_filters = 64, filter_size = 3, stride=1, padding=3, dilation=3)\r\n",
    "        self.dilate3 = fluid.dygraph.Conv2D(\"dilate1\", num_filters = 64, filter_size = 3, stride=1, padding=4, dilation=4)\r\n",
    "        self.dilate4 = fluid.dygraph.Conv2D(\"dilate1\", num_filters = 64, filter_size = 3, stride=1, padding=6, dilation=6)\r\n",
    "        self.bn1 = fluid.dygraph.BatchNorm(\"bn1\",256, act = \"relu\", is_test = is_test)\r\n",
    "        \r\n",
    "        self.merge = fluid.dygraph.Conv2D(\"merge\", num_filters = 256, filter_size = 1, stride=1)\r\n",
    "        self.bn2 = fluid.dygraph.BatchNorm(\"bn2\",256, act = \"relu\", is_test = is_test)\r\n",
    "        \r\n",
    "    def forward(self, inputs):    \r\n",
    "        x1 = self.dilate1(inputs)\r\n",
    "        x2 = self.dilate2(inputs)\r\n",
    "        \r\n",
    "        x3 = self.dilate3(inputs)\r\n",
    "        x4 = self.dilate4(inputs)\r\n",
    "        out = fluid.layers.concat(input=[x1,x2,x3,x4], axis=1)\r\n",
    "        out = self.bn1(out)\r\n",
    "        out = self.merge(out)\r\n",
    "        return self.bn2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Head(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope, clsNum, is_test):\r\n",
    "        super(Head, self).__init__(name_scope)\r\n",
    "        self.cls1 = ASPP(\"cls1\", is_test = is_test)\r\n",
    "        self.cls2 = ASPP(\"cls2\", is_test = is_test)\r\n",
    "        self.cls3 = ASPP(\"cls3\", is_test = is_test)\r\n",
    "        #self.cls4 = ASPP(\"cls4\", is_test = is_test)\r\n",
    "        \r\n",
    "        self.loc1 = ASPP(\"loc1\", is_test = is_test)\r\n",
    "        self.loc2 = ASPP(\"loc2\", is_test = is_test)\r\n",
    "        self.loc3 = ASPP(\"loc3\", is_test = is_test)\r\n",
    "        #self.loc4 = ASPP(\"loc4\", is_test = is_test)\r\n",
    "\r\n",
    "        self.cls_out = fluid.dygraph.Conv2D(\"class\", num_filters = clsNum, filter_size = 3, stride=1, padding=1, dilation=1)\r\n",
    "        self.center_ness = fluid.dygraph.Conv2D(\"center_ness\", num_filters = 1, filter_size = 3, stride=1, padding=1, dilation=1)\r\n",
    "        self.regression = fluid.dygraph.Conv2D(\"regression\", num_filters = 4, filter_size = 3, stride=1, padding=1, dilation=1)\r\n",
    "        \r\n",
    "    def forward(self, inputs):\r\n",
    "        cls = self.cls1(inputs)\r\n",
    "        cls = self.cls2(cls)\r\n",
    "        cls = self.cls3(cls)\r\n",
    "        #cls = self.cls4(cls)\r\n",
    "        \r\n",
    "        cls_out = self.cls_out(cls)\r\n",
    "        center_ness = self.center_ness(cls)\r\n",
    "\r\n",
    "        \r\n",
    "        loc = self.loc1(inputs)\r\n",
    "        loc = self.loc2(loc)\r\n",
    "        loc = self.loc3(loc)\r\n",
    "        #loc = self.loc4(loc)\r\n",
    "        loc = self.regression(loc)\r\n",
    "        \r\n",
    "        return [fluid.layers.sigmoid(cls_out), \r\n",
    "                fluid.layers.sigmoid(center_ness), \r\n",
    "                fluid.layers.exp(loc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope, is_test = False):\r\n",
    "        super(ResNet, self).__init__(name_scope)\r\n",
    "        self.conv1 = fluid.dygraph.Conv2D(\"conv1\", num_filters = 64, filter_size = 7, stride=2, padding=3, dilation=1)\r\n",
    "        self.bn1 = fluid.dygraph.BatchNorm(\"bn1\",64, act = \"relu\", is_test = is_test)\r\n",
    "        self.maxPooling  = fluid.dygraph.Pool2D(\"maxpooling\", pool_size=2, pool_stride = 2, pool_type='max')\r\n",
    "\r\n",
    "        self.block1 = Make_layer(\"block1\", 64, layernums = 3, stride = 1, is_test = is_test)\r\n",
    "        self.block2 = Make_layer(\"block2\", 128, layernums = 4, stride = 2, is_test = is_test)\r\n",
    "        self.block3 = Make_layer(\"block3\", 256, layernums = 6, stride = 2, is_test = is_test)\r\n",
    "        self.block4 = Make_layer(\"block4\", 512, layernums = 3, stride = 2, is_test = is_test) #最后一层的输出是带激活函数的\r\n",
    "        self.fpn = FPN(\"FPN\", is_test = is_test)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = self.bn1(x)\r\n",
    "        x = self.maxPooling(x)\r\n",
    "        x = self.block1(x)\r\n",
    "        #return [x]\r\n",
    "        C3 = self.block2(x)\r\n",
    "        C4 = self.block3(C3)\r\n",
    "        C5 = self.block4(C4)\r\n",
    "        #return [C3, C4, C5]\r\n",
    "        return self.fpn([C3, C4, C5])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with fluid.dygraph.guard():\r\n",
    "class Loss(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope):\r\n",
    "        super(Loss, self).__init__(name_scope)\r\n",
    "        self.balance_weight = fluid.dygraph.to_variable(np.array([6, 5, 2, 1.5, 14,14, 11, 9, 6, 7,10, 3, 6.6, 8, 7, 10,  3, 6, 5, 6], \r\n",
    "                                                        dtype = np.float32)) / 10\r\n",
    "        self.balance_weight.stop_gradient = True\r\n",
    "        self.limit = fluid.dygraph.to_variable(np.array([0], dtype = np.float32))\r\n",
    "        self.limit.stop_gradient = True\r\n",
    "                                   \r\n",
    "    def iou_loss(self, pred, label, reg_mask):\r\n",
    "        #label shape = [b, -1, 4]  l,r,t,b\r\n",
    "        i_h = fluid.layers.elementwise_min(pred[:,:,0], label[:,:,0]) + fluid.layers.elementwise_min(pred[:,:,1], label[:,:,1])\r\n",
    "        i_w = fluid.layers.elementwise_min(pred[:,:,2], label[:,:,2]) + fluid.layers.elementwise_min(pred[:,:,3], label[:,:,3])\r\n",
    "        i_area = fluid.layers.elementwise_mul(i_h, i_w)\r\n",
    "        u_area = fluid.layers.elementwise_mul(pred[:,:,0] + pred[:,:,1],  pred[:,:,2] + pred[:,:,3]) +\\\r\n",
    "                 fluid.layers.elementwise_mul(label[:,:,0] + label[:,:,1],  label[:,:,2] + label[:,:,3])\r\n",
    "        iou = i_area / (u_area - i_area + 1e-7)\r\n",
    "        #mask = fluid.layers.greater_than(label, self.limit)\r\n",
    "        #mask = paddle.fluid.layers.cast(mask, dtype = \"float32\")\r\n",
    "        loss = (1 - iou) * reg_mask\r\n",
    "        return fluid.layers.reduce_sum(loss) / (fluid.layers.reduce_sum(reg_mask) + 1e-5)\r\n",
    "        \r\n",
    "    def centerness_loss(self, pred, label, reg_mask):\r\n",
    "        #label shape  = [b, -1]\r\n",
    "        #采用 L1 loss\r\n",
    "        diff = pred - label\r\n",
    "        loss = fluid.layers.elementwise_mul(diff, diff)\r\n",
    "        loss = fluid.layers.elementwise_mul(loss,  5*label + 0.5)\r\n",
    "        #loss = fluid.layers.pow(diff, factor=4.0)\r\n",
    "        #loss = paddle.fluid.layers.abs(pred - label)\r\n",
    "        #loss = 5*reg_mask*loss + 0.5*(1-reg_mask)*loss\r\n",
    "        #loss = paddle.fluid.layers.cast(loss, dtype = \"float32\")\r\n",
    "        #loss = fluid.layers.clip(loss,1e-4, 1.0)\r\n",
    "        #sum_loss = fluid.layers.reduce_sum(loss).numpy()\r\n",
    "        #print(\"centerness_loss sum = %s, is_nan = %s\"%(sum_loss, np.isnan(sum_loss)))\r\n",
    "        return fluid.layers.reduce_mean(loss) * 10\r\n",
    "                \r\n",
    "    def focal_loss(self, pred, label):\r\n",
    "        #cls loss shape = [b,-1, clsNum]\r\n",
    "        label = (1 - label)*0.005 + label *0.99\r\n",
    "        alpha = 0.95\r\n",
    "        gamma = 3.0\r\n",
    "        eps = 1e-7\r\n",
    "        bce_loss = -1 * (alpha * label * fluid.layers.log(pred + eps)* self.balance_weight + (1 - alpha) * (1 - label) * fluid.layers.log(1 - pred + eps))\r\n",
    "        focal_weight = label * fluid.layers.pow((1 - pred), gamma) + (1 - label)*fluid.layers.pow(pred, gamma)\r\n",
    "        cls_loss = bce_loss * focal_weight \r\n",
    "        return fluid.layers.reduce_mean(cls_loss) * 1000  #focal_loss值太少，故把mean换成sum\r\n",
    "        \r\n",
    "    def forward(self, cls_out, cls_label, reg_out, reg_label, cent_out, cent_label, reg_mask):\r\n",
    "        return self.focal_loss(cls_out, cls_label), \\\r\n",
    "                self.iou_loss(reg_out, reg_label, reg_mask), \\\r\n",
    "                self.centerness_loss(cent_out, cent_label, reg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FCOS(fluid.dygraph.Layer):\r\n",
    "    def __init__(self, name_scope, clsNum, batch = 8, is_trainning = True):\r\n",
    "        super(FCOS, self).__init__(name_scope)\r\n",
    "        \r\n",
    "        self.trainning = is_trainning\r\n",
    "        self.resnet = ResNet(\"ResNet\", is_test = not is_trainning)\r\n",
    "        self.head1 = Head('Head1', clsNum, is_test = not is_trainning) #head1用于前3层 fpn\r\n",
    "        self.head2 = Head('Head2', clsNum, is_test = not is_trainning)#head2用于前3层 fpn\r\n",
    "        self.stride = [8, 16, 32, 64, 128]\r\n",
    "        self.clsNum = clsNum\r\n",
    "        self.batch = batch\r\n",
    "        self.loss =Loss(\"loss\")\r\n",
    "        print(\"FCOS load final\")\r\n",
    "        \r\n",
    "    def forward(self, x, cls_label = None, reg_label = None, cent_label = None, reg_mask = None):\r\n",
    "        feature = self.resnet(x)\r\n",
    "        #Cls = paddle.fluid.layers.zeros(shape = (self.batch, 0, self.clsNum), dtype = \"float32\")\r\n",
    "        #Reg = paddle.fluid.layers.zeros(shape = (self.batch, 0, 4), dtype = \"float32\")\r\n",
    "        #Center = paddle.fluid.layers.zeros(shape = (self.batch, 0), dtype = \"float32\")\r\n",
    "        Cls, Reg, Center = [],[],[]\r\n",
    "        if(self.trainning):\r\n",
    "            for idx, feat in enumerate(feature):\r\n",
    "                if(idx < 3):\r\n",
    "                    cls_out, center_ness, loc = self.head1(feat)\r\n",
    "                else:\r\n",
    "                    cls_out, center_ness, loc = self.head2(feat)\r\n",
    "                cls_out = fluid.layers.transpose(cls_out, perm=[0, 2, 3, 1])\r\n",
    "                cls_out = fluid.layers.reshape(x=cls_out, shape=[self.batch, -1, self.clsNum], inplace=False)\r\n",
    "                Cls.append(cls_out)\r\n",
    "    \r\n",
    "                center_ness = fluid.layers.transpose(center_ness, perm=[0, 2, 3, 1])\r\n",
    "                center_ness = fluid.layers.reshape(x=center_ness, shape=[self.batch, -1], inplace=False)\r\n",
    "                Center.append(center_ness)\r\n",
    "    \r\n",
    "                loc = fluid.layers.transpose(loc, perm=[0, 2, 3, 1])\r\n",
    "                loc = fluid.layers.reshape(x=loc, shape=[self.batch, -1, 4], inplace=False)\r\n",
    "                Reg.append(loc)\r\n",
    "                \r\n",
    "            Reg = fluid.layers.concat(input=Reg, axis=1)\r\n",
    "            Center = fluid.layers.concat(input=Center, axis=1)\r\n",
    "            Cls = fluid.layers.concat(input=Cls, axis=1)\r\n",
    "            return self.loss(Cls, cls_label, Reg, reg_label, Center, cent_label, reg_mask)\r\n",
    "        else:\r\n",
    "            Scores = []\r\n",
    "            for feat in feature:\r\n",
    "                cls_out, center_ness, loc = self.head(feat)\r\n",
    "                cls_out = fluid.layers.transpose(cls_out, perm=[0, 2, 3, 1])\r\n",
    "                center_ness = fluid.layers.transpose(center_ness, perm=[0, 2, 3, 1])\r\n",
    "                loc = fluid.layers.transpose(loc, perm=[0, 2, 3, 1])\r\n",
    "                \r\n",
    "                argmax = fluid.layers.argmax(cls_out, axis=3)\r\n",
    "                score = fluid.layers.reduce_max(cls_out, dim=3, keep_dim = True)\r\n",
    "                Scores.append(score)\r\n",
    "                Cls.append(argmax)\r\n",
    "                Reg.append(loc)\r\n",
    "                Center.append(center_ness)\r\n",
    "            return Cls, Scores, Center, Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encode():\n",
    "    def __init__(self):\n",
    "        self.divide = [100, 200, 300, 500]\n",
    "        self.stride = [8, 16, 32, 64, 128]\n",
    "        self.size2layer = [[125, 306], [63, 153], [32, 77], [16, 39], [8, 20]]\n",
    "        self.layerNum = 5\n",
    "        self.clsNum = 20\n",
    "        \n",
    "    def gt_process(self, gt):\n",
    "        if gt is None:\n",
    "            return [np.zeros((0,5))]*5\n",
    "        meanedge = np.mean((gt[:,2] - gt[:,0],  gt[:,3] - gt[:,1]), axis=0)\n",
    "        \"\"\"\n",
    "        gt1 = gt[meanedge < self.divide[0]]\n",
    "        gt2 = gt[(meanedge >= self.divide[0]) * (meanedge < self.divide[1])]\n",
    "        gt3 = gt[(meanedge >= self.divide[1]) * (meanedge < self.divide[2])]\n",
    "        gt4 = gt[(meanedge >= self.divide[2]) * (meanedge < self.divide[3])]\n",
    "        gt5 = gt[meanedge >= self.divide[3]]\n",
    "        \"\"\"\n",
    "        gt1 = gt\n",
    "        gt2 = gt[meanedge > self.divide[0]]\n",
    "        gt3 = gt[meanedge > self.divide[1]]\n",
    "        gt4 = gt[meanedge > self.divide[2]]\n",
    "        gt5 = gt[meanedge > self.divide[3]]\n",
    "        return [gt1, gt2, gt3, gt4, gt5]\n",
    "        \n",
    "    def encode(self, gt):\n",
    "        # gt为 numpy形式, shape = (N, 5) loc = x1,y1,x2,y2,cls  cls为(0-19)\n",
    "        gt = self.gt_process(gt) #gt为list,存储5个layer的gt\n",
    "        Reg = np.zeros(shape = (0,4))\n",
    "        Center = np.zeros(shape = (0,))\n",
    "        Cls = np.zeros(shape = (0,self.clsNum))\n",
    "        for i in range(self.layerNum):\n",
    "            #if(gt[i].shape[0] == 0):continue\n",
    "            targetReg = np.zeros(shape = self.size2layer[i] + [4], dtype = np.float32)#4分别为（l, r, t, b）\n",
    "            targetCenter = np.zeros(shape = self.size2layer[i], dtype = np.float32)\n",
    "            targetCls = np.zeros(shape = self.size2layer[i] + [self.clsNum], dtype = np.float32)\n",
    "            gt_cls = gt[i][:,4].astype(np.int32)\n",
    "            #print(\"layer = %d, gt[i] = %s\"%(i+1, gt[i]))\n",
    "            targetGt = (gt[i][:,:4]/self.stride[i])\n",
    "            targetGt[:,[2,3]] = np.ceil(targetGt[:,[2,3]])\n",
    "            targetGt[:,[0,1]] = np.floor(targetGt[:,[0,1]])\n",
    "            targetGt = targetGt.astype(np.int32)\n",
    "            #[w, h] = targetGt[:,2] - targetGt[:,0] + 1, targetGt[:,3] - targetGt[:,1] + 1\n",
    "            w, h = targetGt[:,2] - targetGt[:,0], targetGt[:,3] - targetGt[:,1]\n",
    "            for j in range(targetGt.shape[0]):\n",
    "                bbox = targetGt[j]\n",
    "                bboxMap = np.zeros(shape = (h[j], w[j], 4))\n",
    "                bboxMap[:,:,0] = np.arange(w[j]).reshape(1, -1) * np.ones(shape = (h[j], 1))\n",
    "                bboxMap[:,:,1] = w[j] - 1 - bboxMap[:,:,0]\n",
    "                bboxMap[:,:,2] = np.arange(h[j]).reshape(-1, 1) * np.ones(shape = (1, w[j]))\n",
    "                bboxMap[:,:,3] = h[j] - 1 - bboxMap[:,:,2]\n",
    "                bboxMap += 1\n",
    "                #try:\n",
    "                targetReg[bbox[1]:bbox[3], bbox[0]:bbox[2]] = bboxMap\n",
    "                #except:\n",
    "                #    print(\"bbox \",bbox)\n",
    "                #    print(\"targetReg \",targetReg.shape)\n",
    "                #    print(\"bboxMap \",bboxMap.shape)\n",
    "                targetCenter[bbox[1]:bbox[3], bbox[0]:bbox[2]] = np.sqrt(\n",
    "                    (np.minimum(bboxMap[:,:,0], bboxMap[:,:,1]) * np.minimum(bboxMap[:,:,2], bboxMap[:,:,3])) / (np.maximum(bboxMap[:,:,0], bboxMap[:,:,1]) * np.maximum(bboxMap[:,:,2], bboxMap[:,:,3]))\n",
    "                )\n",
    "                targetCls[bbox[1]:bbox[3], bbox[0]:bbox[2], gt_cls[j]] = 1\n",
    "            Reg = np.vstack((Reg, targetReg.reshape(-1, 4)))\n",
    "            Cls = np.vstack((Cls, targetCls.reshape(-1, self.clsNum)))\n",
    "            Center = np.hstack((Center, targetCenter.reshape(-1)))\n",
    "        return Cls, Reg, Center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_id = {\"破洞\":1,\"水渍\":2,\"油渍\":2,\"污渍\":2,\n",
    "               \"三丝\":3,\"结头\":4,\"花板跳\":5,\"百脚\":6,\n",
    "               \"毛粒\":7,\"粗经\":8,\"松经\":9,\"断经\":10,\n",
    "               \"吊经\":11,\"粗维\":12,\"纬缩\":13,\"浆斑\":14,\n",
    "               \"整经结\":15,\"星跳\":16,\"跳花\":16,\"断氨纶\":17,\n",
    "               \"稀密档\":18,\"浪纹档\":18,\"色差档\":18,\"磨痕\":19,\n",
    "               \"轧痕\":19,\"修痕\":19,\"烧毛痕\":19,\"死皱\":20,\n",
    "               \"云织\":20,\"双纬\":20,\"双经\":20,\"跳纱\":20,\n",
    "               \"筘路\":20,\"纬纱不良\":20}\n",
    "category_id = {key:category_id[key]-1 for key in category_id}  \n",
    "img_h = 1000 - 1\n",
    "img_w = 2446 - 1\n",
    "clsType = [i for i in category_id]\n",
    "with open(r\"work/train_data/guangdong1_round1_train1_20190818/Annotations/anno_train.json\",'r') as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "defect_path = r\"work/train_data/guangdong1_round1_train1_20190818/defect_Images\"\n",
    "normal_path = r\"work/train_data/guangdong1_round1_train1_20190818/normal_Images\"\n",
    "\n",
    "id2category = {0:\"破洞\", 1:random.choice([\"水渍\",\"油渍\",\"污渍\"]),\n",
    "               2:\"三丝\", 3:\"结头\", 4:\"花板跳\",\n",
    "               5:\"百脚\", 6:\"毛粒\", 7:\"粗经\",\n",
    "               8:\"松经\", 9:\"断经\",\n",
    "               10:\"吊经\", 11:\"粗维\", 12:\"纬缩\", 13:\"浆斑\",\n",
    "               14:\"整经结\",15:random.choice([\"星跳\",\"跳花\"]),\n",
    "               16:\"断氨纶\",17:random.choice([\"稀密档\",\"浪纹档\",\"色差档\"]),\n",
    "               18:random.choice([\"磨痕\",\"轧痕\",\"修痕\",\"烧毛痕\"]),\n",
    "               19:random.choice([\"死皱\",\"云织\",\"双纬\",\"双经\",\"跳纱\",\"筘路\",\"纬纱不良\"])}\n",
    "               \n",
    "def bufferGenerater(js, defect_path, size = 3):\n",
    "    buffer = {i:[] for i in category_id}    #buffer中为【x1,y1,x2,y2,w,h】\n",
    "    total = len(buffer) * size\n",
    "    count = 0\n",
    "    for info in js:\n",
    "        defect_name = info[\"defect_name\"]\n",
    "        if(len(buffer[defect_name]) < size):\n",
    "            count += 1\n",
    "            img = cv2.imread(os.path.join(defect_path, info[\"name\"]))\n",
    "            bbox = [int(i) for i in info[\"bbox\"]]\n",
    "            #if(bbox[2]>2445):bbox[2] = 2445\n",
    "            #if(bbox[3]> 999):bbox[3] = 999\n",
    "            dat = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "            buffer[defect_name].append(dat)\n",
    "        if count >= total:break\n",
    "    return buffer\n",
    "Buffer = bufferGenerater(js, defect_path, size = 5)\n",
    "\n",
    "def js2label(js, normlist):\n",
    "    label = {}\n",
    "    for info in js:\n",
    "        if not info[\"name\"] in label:\n",
    "            label[info[\"name\"]] = []\n",
    "        label[info[\"name\"]].append(info[\"bbox\"] + [category_id[info[\"defect_name\"]]])\n",
    "        \n",
    "    for key in label:\n",
    "        temp = np.array(label[key]) #将bbox转为numpy类型,并去小数\n",
    "        temp[:,[2,3]] = np.ceil(temp[:,[2,3]])\n",
    "        temp[:,2] = np.clip(temp[:,2], 0, 2445)\n",
    "        temp[:,3] = np.clip(temp[:,3], 0, 999)\n",
    "        temp[:,[0,1]] = np.floor(temp[:,[0,1]])\n",
    "        label[key] = temp.astype(np.int32) \n",
    "\n",
    "    for imgname in normlist:\n",
    "        label[imgname] = None\n",
    "    return label\n",
    "\n",
    "encode = Encode() ##实例化Encode\n",
    "\n",
    "mirror = 0.35\n",
    "flip = 0.35\n",
    "mixup = 0.35\n",
    "def aug(img, anno):\n",
    "    #draw(img.copy(), anno.copy())\n",
    "    #print(\"after aug \",anno)\n",
    "    if(random.random() < mirror):\n",
    "        img = cv2.flip(img, 1)\n",
    "        if(anno.shape[0] > 0):\n",
    "            w = anno[:,2] - anno[:,0]\n",
    "            anno[:,2] = img_w - anno[:,0] + 1\n",
    "            anno[:,0] = anno[:,2] - w\n",
    "        \n",
    "    if(random.random() < flip):\n",
    "        img = cv2.flip(img, 0)\n",
    "        if(anno.shape[0] > 0):\n",
    "            h = anno[:,3] - anno[:,1]\n",
    "            anno[:,3] = img_h - anno[:,1] + 1\n",
    "            anno[:,1] = anno[:,3] - h\n",
    "    #draw(img.copy(), anno.copy())\n",
    "    #print(\"befor aug \",anno)\n",
    "    return img,anno\n",
    "\n",
    "def normalImgAddSample(img):\n",
    "    global Buffer\n",
    "    temp = np.zeros_like(img, dtype = np.int32)\n",
    "    highlight = (img>220)\n",
    "    img_mean = np.mean(img, axis = (0,1))\n",
    "    annos = []\n",
    "    for i in range(5):\n",
    "        add_type = random.choice(clsType)\n",
    "        dif = []\n",
    "        for buf_img in Buffer[add_type]:\n",
    "            mean = np.mean(np.array(buf_img), axis = (0,1))\n",
    "            dif.append(np.sum((img_mean - mean)**2))\n",
    "        index = np.argmin(dif)\n",
    "        add_img = Buffer[add_type][index]\n",
    "        [h,w] = add_img.shape[:2]\n",
    "        \n",
    "        if(h >= img_h):index_h = 0\n",
    "        else:index_h = random.randint(0,img_h - h)\n",
    "\n",
    "        if(w >= img_w):index_w = 0     \n",
    "        else:index_w = random.randint(0,img_w - w)\n",
    "\n",
    "        if(np.sum(temp[index_h:index_h + h, index_w:index_w + w]) == 0 and \\\n",
    "           np.sum(highlight[index_h:index_h + h, index_w:index_w + w]) <= 0.1 * h * w):\n",
    "            cover_area_mean = np.mean(img[index_h:index_h + h, index_w:index_w + w], axis = (0,1))\n",
    "            #if(np.mean(cover_area_mean) > 170):continue\n",
    "            temp[index_h:index_h + h, index_w:index_w + w] = add_img * (cover_area_mean / np.mean(add_img, axis = (0,1)))\n",
    "            bbox = [index_w, index_h, w, h, category_id[add_type]]\n",
    "            annos.append(bbox)\n",
    "    \n",
    "    if(len(annos) == 0):return img, np.zeros((0,5))\n",
    "    annos = np.array(annos)\n",
    "    annos[:,2] += annos[:,0]\n",
    "    annos[:,3] += annos[:,1]\n",
    "    img = img * (temp == 0) + temp \n",
    "    return img,annos\n",
    " \n",
    "\n",
    "def generator(defect_path = None, normal_path = None, label = None, imgnames = None, normlist = None):\n",
    "    def __generater__():\n",
    "        for name in imgnames:\n",
    "            if False:\n",
    "            #if label[name] is None:\n",
    "                x = cv2.imread(os.path.join(normal_path, name))\n",
    "                x,y = normalImgAddSample(x)\n",
    "            else:\n",
    "                x = cv2.imread(os.path.join(defect_path, name))\n",
    "                y = label[name]\n",
    "                \n",
    "                \"\"\"\n",
    "                if(random.random() < 0.05):\n",
    "                    index = y.shape[0]%3 - 1\n",
    "                    cls = id2category[y[index][4]]\n",
    "                    Buffer[cls].pop(0)\n",
    "                    bbox = y[index]\n",
    "                    Buffer[cls].append(x[bbox[1]:bbox[3], bbox[0]:bbox[2]])\n",
    "                \"\"\"\n",
    "                if(random.random() < mixup):\n",
    "                    tempImg = cv2.imread(os.path.join(normal_path, random.choice(normlist)))\n",
    "                    x = (x*0.95 + tempImg*0.05).astype(np.uint8)\n",
    "            x,y = aug(x, y)\n",
    "            Cls, Reg, Center = encode.encode(y)\n",
    "            yield x.transpose(2,0,1), Cls, Reg, Center\n",
    "            \n",
    "    return __generater__\n",
    "   \n",
    "label = js2label(js, os.listdir(r\"work/train_data/guangdong1_round1_train1_20190818/normal_Images\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import copy\n",
    "#train_reader = generator(defect_path=defect_path, normal_path=normal_path, label=label)\n",
    "#for idx, dat in enumerate(train_reader()):\n",
    "#    data1 = copy.deepcopy(dat)\n",
    "#    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCOS load final\n",
      "training begin.....\n",
      "epoch = 3 | iter = 0 | loss = 0.78494 | focal_loss = 0.15311 | iou_loss = 0.62012 | centerness_loss = 0.01172 | use time = 9.829 s\n",
      "epoch = 3 | iter = 100 | loss = 1.09526 | focal_loss = 0.52090 | iou_loss = 0.47156 | centerness_loss = 0.10281 | use time = 79.282 s\n",
      "epoch = 3 | iter = 200 | loss = 1.28634 | focal_loss = 0.67750 | iou_loss = 0.47749 | centerness_loss = 0.13135 | use time = 81.977 s\n",
      "epoch = 3 | iter = 300 | loss = 1.35549 | focal_loss = 0.71561 | iou_loss = 0.48337 | centerness_loss = 0.15651 | use time = 81.971 s\n",
      "epoch = 3 | iter = 400 | loss = 1.31031 | focal_loss = 0.68627 | iou_loss = 0.46967 | centerness_loss = 0.15437 | use time = 84.541 s\n",
      "epoch = 3 | iter = 500 | loss = 1.16852 | focal_loss = 0.56708 | iou_loss = 0.45865 | centerness_loss = 0.14279 | use time = 81.978 s\n",
      "epoch = 3 | iter = 600 | loss = 1.08012 | focal_loss = 0.51226 | iou_loss = 0.45276 | centerness_loss = 0.11510 | use time = 84.064 s\n",
      "epoch = 3 | iter = 700 | loss = 1.10258 | focal_loss = 0.51933 | iou_loss = 0.46034 | centerness_loss = 0.12291 | use time = 82.989 s\n",
      "epoch = 3 | iter = 800 | loss = 1.13903 | focal_loss = 0.54945 | iou_loss = 0.46755 | centerness_loss = 0.12203 | use time = 82.257 s\n",
      "epoch = 3 | iter = 900 | loss = 1.28781 | focal_loss = 0.68328 | iou_loss = 0.46446 | centerness_loss = 0.14006 | use time = 82.643 s\n",
      "epoch = 3 | iter = 1000 | loss = 1.39485 | focal_loss = 0.75595 | iou_loss = 0.48105 | centerness_loss = 0.15785 | use time = 82.989 s\n",
      "epoch = 3 | iter = 1100 | loss = 1.50445 | focal_loss = 0.82586 | iou_loss = 0.48295 | centerness_loss = 0.19563 | use time = 86.712 s\n",
      "epoch = 3 | iter = 1200 | loss = 1.35961 | focal_loss = 0.68922 | iou_loss = 0.48630 | centerness_loss = 0.18408 | use time = 85.577 s\n",
      "epoch = 3 | iter = 1300 | loss = 1.38723 | focal_loss = 0.73005 | iou_loss = 0.47211 | centerness_loss = 0.18508 | use time = 85.171 s\n",
      "epoch = 3 | iter = 1400 | loss = 1.29262 | focal_loss = 0.67303 | iou_loss = 0.47001 | centerness_loss = 0.14958 | use time = 84.714 s\n",
      "epoch = 3 | iter = 1500 | loss = 1.27140 | focal_loss = 0.65605 | iou_loss = 0.46325 | centerness_loss = 0.15211 | use time = 83.625 s\n",
      "epoch = 3 | iter = 1600 | loss = 1.22280 | focal_loss = 0.60057 | iou_loss = 0.47419 | centerness_loss = 0.14804 | use time = 85.175 s\n",
      "epoch = 3 | iter = 1700 | loss = 1.26984 | focal_loss = 0.63283 | iou_loss = 0.47268 | centerness_loss = 0.16433 | use time = 82.793 s\n",
      "epoch = 3 | iter = 1800 | loss = 1.22523 | focal_loss = 0.59743 | iou_loss = 0.47771 | centerness_loss = 0.15009 | use time = 83.927 s\n",
      "epoch = 3 | iter = 1900 | loss = 1.23143 | focal_loss = 0.61381 | iou_loss = 0.47244 | centerness_loss = 0.14517 | use time = 81.667 s\n",
      "epoch = 3 | iter = 2000 | loss = 1.21469 | focal_loss = 0.60474 | iou_loss = 0.46755 | centerness_loss = 0.14240 | use time = 84.652 s\n",
      "epoch = 3 | iter = 2100 | loss = 1.33155 | focal_loss = 0.69175 | iou_loss = 0.47287 | centerness_loss = 0.16692 | use time = 83.782 s\n",
      "epoch = 3 | iter = 2200 | loss = 1.35248 | focal_loss = 0.70430 | iou_loss = 0.47380 | centerness_loss = 0.17438 | use time = 83.801 s\n",
      "epoch = 3 | iter = 2300 | loss = 1.36630 | focal_loss = 0.70826 | iou_loss = 0.48270 | centerness_loss = 0.17534 | use time = 84.888 s\n",
      "epoch = 3 | iter = 2400 | loss = 1.37235 | focal_loss = 0.72059 | iou_loss = 0.47870 | centerness_loss = 0.17307 | use time = 83.006 s\n",
      "epoch = 3 | iter = 2500 | loss = 1.36478 | focal_loss = 0.70898 | iou_loss = 0.48519 | centerness_loss = 0.17061 | use time = 82.843 s\n",
      "epoch = 3 | iter = 2600 | loss = 1.38506 | focal_loss = 0.72090 | iou_loss = 0.48644 | centerness_loss = 0.17772 | use time = 84.722 s\n",
      "epoch = 3 | iter = 2700 | loss = 1.28891 | focal_loss = 0.64824 | iou_loss = 0.47372 | centerness_loss = 0.16694 | use time = 84.757 s\n",
      "epoch = 3 | iter = 2800 | loss = 1.41007 | focal_loss = 0.72815 | iou_loss = 0.48673 | centerness_loss = 0.19519 | use time = 83.810 s\n",
      "epoch = 3 | iter = 2900 | loss = 1.38267 | focal_loss = 0.71697 | iou_loss = 0.47731 | centerness_loss = 0.18838 | use time = 83.119 s\n",
      "epoch = 3 | iter = 3000 | loss = 1.53587 | focal_loss = 0.82478 | iou_loss = 0.49971 | centerness_loss = 0.21137 | use time = 86.458 s\n",
      "epoch = 3 | iter = 3100 | loss = 1.33898 | focal_loss = 0.68837 | iou_loss = 0.47529 | centerness_loss = 0.17532 | use time = 83.210 s\n",
      "epoch = 3 | iter = 3200 | loss = 1.33463 | focal_loss = 0.68918 | iou_loss = 0.47702 | centerness_loss = 0.16843 | use time = 83.909 s\n",
      "epoch = 3 | iter = 3300 | loss = 1.22172 | focal_loss = 0.62119 | iou_loss = 0.45739 | centerness_loss = 0.14313 | use time = 86.928 s\n",
      "epoch = 3 | iter = 3400 | loss = 1.38481 | focal_loss = 0.76266 | iou_loss = 0.45636 | centerness_loss = 0.16579 | use time = 84.257 s\n",
      "epoch = 3 | iter = 3500 | loss = 1.26032 | focal_loss = 0.66428 | iou_loss = 0.44892 | centerness_loss = 0.14712 | use time = 85.915 s\n",
      "epoch = 3 | iter = 3600 | loss = 1.26777 | focal_loss = 0.66150 | iou_loss = 0.45260 | centerness_loss = 0.15367 | use time = 87.499 s\n",
      "epoch = 3 | iter = 3700 | loss = 1.12834 | focal_loss = 0.53993 | iou_loss = 0.45122 | centerness_loss = 0.13719 | use time = 83.133 s\n",
      "epoch = 3 | iter = 3800 | loss = 1.19977 | focal_loss = 0.59086 | iou_loss = 0.45438 | centerness_loss = 0.15453 | use time = 85.622 s\n",
      "epoch = 3 | iter = 3900 | loss = 1.24321 | focal_loss = 0.61408 | iou_loss = 0.46581 | centerness_loss = 0.16332 | use time = 85.061 s\n",
      "epoch = 3 | iter = 4000 | loss = 1.29427 | focal_loss = 0.65186 | iou_loss = 0.48089 | centerness_loss = 0.16151 | use time = 84.617 s\n",
      "epoch = 3 | iter = 4100 | loss = 1.34078 | focal_loss = 0.69020 | iou_loss = 0.49551 | centerness_loss = 0.15506 | use time = 83.439 s\n",
      "epoch = 3 | iter = 4200 | loss = 1.29316 | focal_loss = 0.65373 | iou_loss = 0.48849 | centerness_loss = 0.15094 | use time = 82.336 s\n",
      "epoch = 3 | iter = 4300 | loss = 1.28408 | focal_loss = 0.65890 | iou_loss = 0.47549 | centerness_loss = 0.14968 | use time = 87.278 s\n",
      "epoch = 3 | iter = 4400 | loss = 1.27228 | focal_loss = 0.65040 | iou_loss = 0.46051 | centerness_loss = 0.16138 | use time = 82.795 s\n",
      "epoch = 3 | iter = 4500 | loss = 1.35938 | focal_loss = 0.70618 | iou_loss = 0.47421 | centerness_loss = 0.17898 | use time = 84.176 s\n",
      "epoch = 3 | iter = 4600 | loss = 1.41024 | focal_loss = 0.73311 | iou_loss = 0.48505 | centerness_loss = 0.19208 | use time = 82.599 s\n",
      "epoch = 3 | iter = 4700 | loss = 1.39505 | focal_loss = 0.72540 | iou_loss = 0.49375 | centerness_loss = 0.17590 | use time = 83.427 s\n",
      "epoch = 3 | iter = 4800 | loss = 1.27585 | focal_loss = 0.66356 | iou_loss = 0.46966 | centerness_loss = 0.14263 | use time = 86.115 s\n",
      "epoch = 3 | iter = 4900 | loss = 1.18411 | focal_loss = 0.59279 | iou_loss = 0.46275 | centerness_loss = 0.12857 | use time = 82.329 s\n",
      "epoch = 3 | iter = 5000 | loss = 1.15016 | focal_loss = 0.56666 | iou_loss = 0.45427 | centerness_loss = 0.12923 | use time = 82.178 s\n",
      "epoch = 3 | iter = 5100 | loss = 1.17684 | focal_loss = 0.57267 | iou_loss = 0.46990 | centerness_loss = 0.13427 | use time = 84.841 s\n",
      "epoch = 3 | iter = 5200 | loss = 1.14934 | focal_loss = 0.55686 | iou_loss = 0.46379 | centerness_loss = 0.12868 | use time = 83.199 s\n",
      "epoch = 4 | iter = 0 | loss = 1.22835 | focal_loss = 0.60901 | iou_loss = 0.47436 | centerness_loss = 0.14498 | use time = 62.946 s\n",
      "epoch = 4 | iter = 100 | loss = 1.38928 | focal_loss = 0.70056 | iou_loss = 0.53074 | centerness_loss = 0.15797 | use time = 83.618 s\n",
      "epoch = 4 | iter = 200 | loss = 1.46601 | focal_loss = 0.70931 | iou_loss = 0.60203 | centerness_loss = 0.15468 | use time = 85.835 s\n",
      "epoch = 4 | iter = 300 | loss = 1.52473 | focal_loss = 0.71454 | iou_loss = 0.65500 | centerness_loss = 0.15519 | use time = 83.311 s\n",
      "epoch = 4 | iter = 400 | loss = 1.47772 | focal_loss = 0.67396 | iou_loss = 0.64933 | centerness_loss = 0.15443 | use time = 85.873 s\n",
      "epoch = 4 | iter = 500 | loss = 1.35341 | focal_loss = 0.59848 | iou_loss = 0.61636 | centerness_loss = 0.13857 | use time = 86.126 s\n",
      "epoch = 4 | iter = 600 | loss = 1.36801 | focal_loss = 0.60380 | iou_loss = 0.62046 | centerness_loss = 0.14375 | use time = 85.680 s\n",
      "epoch = 4 | iter = 700 | loss = 1.47110 | focal_loss = 0.66619 | iou_loss = 0.64725 | centerness_loss = 0.15765 | use time = 85.720 s\n",
      "epoch = 4 | iter = 800 | loss = 1.67326 | focal_loss = 0.79870 | iou_loss = 0.67941 | centerness_loss = 0.19516 | use time = 88.869 s\n",
      "epoch = 4 | iter = 900 | loss = 1.75333 | focal_loss = 0.85708 | iou_loss = 0.68188 | centerness_loss = 0.21438 | use time = 84.894 s\n",
      "epoch = 4 | iter = 1000 | loss = 1.69983 | focal_loss = 0.82408 | iou_loss = 0.67460 | centerness_loss = 0.20115 | use time = 86.283 s\n",
      "epoch = 4 | iter = 1100 | loss = 1.79706 | focal_loss = 0.90275 | iou_loss = 0.67979 | centerness_loss = 0.21451 | use time = 86.631 s\n",
      "epoch = 4 | iter = 1200 | loss = 1.68395 | focal_loss = 0.82602 | iou_loss = 0.67222 | centerness_loss = 0.18571 | use time = 87.760 s\n",
      "epoch = 4 | iter = 1300 | loss = 1.77594 | focal_loss = 0.88512 | iou_loss = 0.68054 | centerness_loss = 0.21028 | use time = 86.015 s\n",
      "epoch = 4 | iter = 1400 | loss = 1.58982 | focal_loss = 0.74497 | iou_loss = 0.66043 | centerness_loss = 0.18443 | use time = 84.124 s\n",
      "epoch = 4 | iter = 1500 | loss = 1.64923 | focal_loss = 0.79236 | iou_loss = 0.66284 | centerness_loss = 0.19403 | use time = 85.239 s\n",
      "epoch = 4 | iter = 1600 | loss = 1.51839 | focal_loss = 0.69959 | iou_loss = 0.64893 | centerness_loss = 0.16987 | use time = 88.265 s\n",
      "epoch = 4 | iter = 1700 | loss = 1.60036 | focal_loss = 0.74974 | iou_loss = 0.66039 | centerness_loss = 0.19023 | use time = 82.784 s\n",
      "epoch = 4 | iter = 1800 | loss = 1.55700 | focal_loss = 0.69957 | iou_loss = 0.66997 | centerness_loss = 0.18746 | use time = 83.396 s\n",
      "epoch = 4 | iter = 1900 | loss = 1.57872 | focal_loss = 0.71007 | iou_loss = 0.67622 | centerness_loss = 0.19243 | use time = 83.844 s\n",
      "epoch = 4 | iter = 2000 | loss = 1.56174 | focal_loss = 0.71914 | iou_loss = 0.66944 | centerness_loss = 0.17316 | use time = 83.862 s\n",
      "epoch = 4 | iter = 2100 | loss = 1.58040 | focal_loss = 0.74624 | iou_loss = 0.66749 | centerness_loss = 0.16666 | use time = 83.988 s\n",
      "epoch = 4 | iter = 2200 | loss = 1.60276 | focal_loss = 0.77124 | iou_loss = 0.66799 | centerness_loss = 0.16352 | use time = 84.190 s\n",
      "epoch = 4 | iter = 2300 | loss = 1.62789 | focal_loss = 0.78037 | iou_loss = 0.67640 | centerness_loss = 0.17112 | use time = 83.068 s\n",
      "epoch = 4 | iter = 2400 | loss = 1.66535 | focal_loss = 0.80436 | iou_loss = 0.67042 | centerness_loss = 0.19057 | use time = 84.011 s\n",
      "epoch = 4 | iter = 2500 | loss = 1.63650 | focal_loss = 0.78701 | iou_loss = 0.66057 | centerness_loss = 0.18891 | use time = 90.128 s\n",
      "epoch = 4 | iter = 2600 | loss = 1.65473 | focal_loss = 0.78982 | iou_loss = 0.65557 | centerness_loss = 0.20934 | use time = 81.626 s\n",
      "epoch = 4 | iter = 2700 | loss = 1.61006 | focal_loss = 0.75805 | iou_loss = 0.64884 | centerness_loss = 0.20317 | use time = 82.562 s\n",
      "epoch = 4 | iter = 2800 | loss = 1.59761 | focal_loss = 0.74356 | iou_loss = 0.65588 | centerness_loss = 0.19818 | use time = 83.775 s\n",
      "epoch = 4 | iter = 2900 | loss = 1.49691 | focal_loss = 0.67044 | iou_loss = 0.66209 | centerness_loss = 0.16437 | use time = 82.455 s\n",
      "epoch = 4 | iter = 3000 | loss = 1.55977 | focal_loss = 0.70715 | iou_loss = 0.68337 | centerness_loss = 0.16926 | use time = 83.945 s\n",
      "epoch = 4 | iter = 3100 | loss = 1.57354 | focal_loss = 0.70799 | iou_loss = 0.69425 | centerness_loss = 0.17129 | use time = 83.331 s\n",
      "epoch = 4 | iter = 3200 | loss = 1.54861 | focal_loss = 0.71342 | iou_loss = 0.67180 | centerness_loss = 0.16339 | use time = 83.223 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import paddle.fluid as fluid\n",
    "import paddle \n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(r\"work/fcos\")\n",
    "from FCOS import FCOS\n",
    "from data import datGenerator\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 15\n",
    "learning_rate = 3e-5\n",
    "start = time.time()\n",
    "cls_loss_hist = collections.deque(maxlen=300)\n",
    "reg_loss_hist = collections.deque(maxlen=300)\n",
    "cen_loss_hist = collections.deque(maxlen=300)\n",
    "\n",
    "#train_reader = generator(defect_path=defect_path, normal_path=normal_path, label=label, imgnames = imgnames, normlist = normlist)\n",
    "#train_reader = paddle.reader.shuffle(train_reader, buf_size=32)\n",
    "#train_reader = paddle.batch(train_reader, batch_size= batch_size,drop_last=False)\n",
    "train_generator = datGenerator()\n",
    "train_reader = train_generator.generator\n",
    "train_reader = paddle.batch(train_reader, batch_size= batch_size,drop_last=False)\n",
    "\n",
    "with fluid.dygraph.guard():\n",
    "    lr = fluid.layers.cosine_decay( learning_rate = learning_rate, step_each_epoch=4774, epochs=epochs)\n",
    "    adam = fluid.optimizer.AdamOptimizer(learning_rate = lr)\n",
    "    model = FCOS(\"fcos\", 20, batch=batch_size)\n",
    "    parameters, adam_parm = fluid.dygraph.load_persistables(\"work/Model/model1/epochs=2\")\n",
    "    adam.load(adam_parm)\n",
    "    model.load_dict(parameters)\n",
    "    print(\"training begin.....\")\n",
    "    for epoch in range(3,epochs):\n",
    "        for idx, data in enumerate(train_reader()):\n",
    "            img = fluid.dygraph.to_variable(np.stack([dat[0] for dat in data], axis=0).astype(np.float32))\n",
    "            cls = fluid.dygraph.to_variable(np.stack([dat[1] for dat in data], axis=0).astype(np.float32))\n",
    "            reg = np.stack([dat[2] for dat in data], axis=0)\n",
    "            reg_mask = (reg[:,:,0]> 0)\n",
    "            reg = fluid.dygraph.to_variable(reg.astype(np.float32))\n",
    "            reg_mask = fluid.dygraph.to_variable(reg_mask.astype(np.float32))\n",
    "            cen = fluid.dygraph.to_variable(np.stack([dat[3] for dat in data], axis=0).astype(np.float32))\n",
    "            cls.stop_gradient = True\n",
    "            reg.stop_gradient = True\n",
    "            cen.stop_gradient = True\n",
    "            reg_mask.stop_gradient = True\n",
    "    \n",
    "            focal_loss, iou_loss, centerness_loss = model(img, cls, reg, cen, reg_mask)\n",
    "            loss = focal_loss + iou_loss + centerness_loss\n",
    "            loss.backward()\n",
    "            adam.minimize(loss)\n",
    "            cls_loss_hist.append(focal_loss.numpy())\n",
    "            reg_loss_hist.append(iou_loss.numpy())\n",
    "            cen_loss_hist.append(centerness_loss.numpy())\n",
    "            if(idx % 100 == 0):\n",
    "                cls_loss_mean = np.mean(cls_loss_hist)\n",
    "                reg_loss_mean = np.mean(reg_loss_hist)\n",
    "                cen_loss_mean = np.mean(cen_loss_hist)\n",
    "                loss_mean = cls_loss_mean + reg_loss_mean + cen_loss_mean\n",
    "                print(\"epoch = %d | iter = %d | loss = %.5f | focal_loss = %.5f | iou_loss = %.5f | centerness_loss = %.5f | use time = %.3f s\"%(epoch, idx, loss_mean, cls_loss_mean, reg_loss_mean, cen_loss_mean, time.time() - start))\n",
    "                start = time.time()\n",
    "            model.clear_gradients()\n",
    "        fluid.dygraph.save_persistables(model.state_dict(), \"work/Model/model1/epochs=%s\"%epoch, optimizers = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:31:37 2019\n",
      "FCOS load final\n",
      "No optimizer loaded. If you didn't save optimizer, please ignore this. The program can still work with new optimizer. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages/ipykernel_launcher.py:96: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271.35309386253357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import paddle.fluid as fluid\n",
    "import paddle \n",
    "import cv2\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import sys\n",
    "sys.path.append(r\"work/fcos\")\n",
    "from FCOS import FCOS\n",
    "print(time.asctime( time.localtime(time.time()) )[11:])\n",
    "batch_size = 1\n",
    "def nms(data, threshold):\n",
    "    #data type = numpy ,shape = N,6   [x1, y1, x2, y2, score, cls]\n",
    "    #print(\"before \",data)\n",
    "    scores = data[:,4]\n",
    "    x1 = data[:,0]\n",
    "    y1 = data[:,1]\n",
    "    x2 = data[:,2]\n",
    "    y2 = data[:,3]\n",
    "    areas = (y2 - y1 + 1) * (x2 - x1 + 1)\n",
    "    index = scores.argsort()[::-1]\n",
    "    res = []\n",
    "    while index.shape[0] >0:\n",
    "        bbox = data[index[0]]\n",
    "        res.append(bbox)\n",
    "        x11 = np.maximum(bbox[0], x1[index[1:]])\n",
    "        y11 = np.maximum(bbox[1], y1[index[1:]])\n",
    "        x22 = np.minimum(bbox[2], x2[index[1:]])\n",
    "        y22 = np.maximum(bbox[3], y2[index[1:]])\n",
    "        \n",
    "        w = np.maximum(0, x22 - x11 + 1) \n",
    "        h = np.maximum(0, y22 - y11 + 1)\n",
    "        i = w*h\n",
    "        iou = i / (areas[index[0]] + areas[index[1:]] - i)\n",
    "        idx = iou < threshold\n",
    "        index = index[1:][idx]\n",
    "        \n",
    "    #print(\"after \",np.array(res))   \n",
    "    return res\n",
    "\n",
    "def testGenerator(testPath):\n",
    "    testList = os.listdir(testPath)\n",
    "    #with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train.json\", 'r') as f:\n",
    "    #        js = json.load(f)\n",
    "    #testList = []\n",
    "    #testPath = r\"work/train_dat/guangdong1_round1_train1_20190818/defect_Images\"\n",
    "    #for info in js:\n",
    "    #    if info[\"name\"] not in testList:\n",
    "    #        testList.append(info[\"name\"])\n",
    "    def __testImg__():\n",
    "        for imgname in testList:\n",
    "            img = cv2.imread(os.path.join(testPath, imgname))\n",
    "            yield img.transpose(2,0,1), imgname\n",
    "    return  __testImg__\n",
    "    \n",
    "test_reader = testGenerator(r\"work/test_data/guangdong1_round1_testA_20190818\")\n",
    "test_reader = paddle.batch(test_reader, batch_size= 2)\n",
    "\n",
    "start = time.time()\n",
    "with fluid.dygraph.guard():\n",
    "    model = FCOS(\"fcos\", 20, batch=batch_size, is_trainning=False)\n",
    "    parameters, _ = fluid.dygraph.load_persistables(\"work/model/v4/7head_eps = 10_half\")\n",
    "    model.load_dict(parameters)\n",
    "    model.eval()\n",
    "    res = {}\n",
    "    for testData in test_reader():\n",
    "        img = fluid.dygraph.to_variable(np.stack([dat[0] for dat in testData], axis=0).astype(np.float32))\n",
    "        imgname = [dat[1] for dat in testData]\n",
    "        Cls, Scores, Centerness, Loc = model(img)\n",
    "        for i in imgname:\n",
    "            res[i] = np.zeros(shape = (0,6))  #[x1,y1,x2,y2, score, cls]\n",
    "        feat_size = [(125, 306), (63, 153), (32, 77), (16, 39), (8, 20)]\n",
    "        stride = [8, 16, 32, 64, 128]\n",
    "        threshold = 0.45\n",
    "\n",
    "        for i in range(5):\n",
    "            score = np.squeeze( (Scores[i] * Centerness[i]).numpy(), axis = 3 )\n",
    "            #score = np.squeeze( Scores[i].numpy(), axis = 3 )\n",
    "            idx = score > threshold\n",
    "            cls = Cls[i].numpy()#[:,np.newaxis,:,:]\n",
    "            reg = Loc[i].numpy()# * stride[i]\n",
    "            for batch_idx in range(batch_size):\n",
    "                b_idx = idx[batch_idx]\n",
    "                b_cls = cls[batch_idx]\n",
    "                b_reg = reg[batch_idx]\n",
    "                bbox = np.zeros_like(b_reg)\n",
    "                bbox[:,:,0] = np.arange(feat_size[i][1]).reshape(1,-1) - b_reg[:,:,0]\n",
    "                bbox[:,:,1] = np.arange(feat_size[i][0]).reshape(-1,1) - b_reg[:,:,2]\n",
    "                bbox[:,:,2] = np.arange(feat_size[i][1]).reshape(1,-1) + b_reg[:,:,1]\n",
    "                bbox[:,:,3] = np.arange(feat_size[i][0]).reshape(-1,1) + b_reg[:,:,3]\n",
    "                bbox = np.dstack([bbox * stride[i], score[batch_idx], b_cls])\n",
    "                bbox = bbox[b_idx]\n",
    "                bbox = np.clip(bbox,0,2446)\n",
    "                bbox[:,3] = np.clip(bbox[:,3],0,1000)\n",
    "                res[imgname[batch_idx]] = np.vstack((res[imgname[batch_idx]], bbox))\n",
    "                \n",
    "    ans = []\n",
    "    for key in res:\n",
    "        if(res[key].shape[0]>0):\n",
    "            bbox = nms(res[key], 0.2)\n",
    "            bbox = np.round(bbox,2)\n",
    "            for i in range(len(bbox)):\n",
    "                temp = {}\n",
    "                temp[\"name\"] = key\n",
    "                temp[\"category\"] = int(bbox[i][5]) + 1\n",
    "                temp[\"bbox\"] = list(bbox[i][:4])\n",
    "                temp[\"score\"] = bbox[i][4]\n",
    "                ans.append(temp)\n",
    "                \n",
    "    with open('work/res/eps10_half.json', 'w') as fp:\n",
    "        json.dump(ans, fp, indent=4, separators=(',', ': '))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total =  5913\n",
      "idx = 600, mean_times = 3\n",
      "idx = 800, mean_times = 3\n",
      "idx = 1000, mean_times = 3\n",
      "idx = 1200, mean_times = 3\n",
      "idx = 1400, mean_times = 3\n",
      "idx = 1600, mean_times = 3\n",
      "idx = 1800, mean_times = 3\n",
      "idx = 2000, mean_times = 3\n",
      "idx = 2200, mean_times = 3\n",
      "idx = 2800, mean_times = 3\n",
      "idx = 3000, mean_times = 3\n",
      "idx = 3200, mean_times = 3\n",
      "idx = 3600, mean_times = 3\n",
      "idx = 4000, mean_times = 3\n",
      "idx = 4200, mean_times = 3\n",
      "idx = 4400, mean_times = 3\n",
      "idx = 4600, mean_times = 3\n",
      "idx = 5000, mean_times = 3\n",
      "idx = 5200, mean_times = 3\n",
      "idx = 5600, mean_times = 3\n",
      "idx = 5800, mean_times = 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import json\r\n",
    "import os\r\n",
    "import random\r\n",
    "from matplotlib import pyplot as plt \r\n",
    "import copy\r\n",
    "import collections\r\n",
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train.json\",'r') as f:\r\n",
    "    js_old = json.load(f)\r\n",
    "    \r\n",
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train2.json\",'r') as f:\r\n",
    "    js_new = json.load(f)\r\n",
    "    \r\n",
    "defect_path = r\"work/train_dat/guangdong1_round1_train1_20190818/defect_Images\"\r\n",
    "normal_path = r\"work/train_dat/guangdong1_round1_train1_20190818/normal_Images\"\r\n",
    "js = js_old + js_new\r\n",
    "\r\n",
    "category2id = {\"破洞\":1,\"水渍\":2,\"油渍\":2,\"污渍\":2,\r\n",
    "               \"三丝\":3,\"结头\":4,\"花板跳\":5,\"百脚\":6,\r\n",
    "               \"毛粒\":7,\"粗经\":8,\"松经\":9,\"断经\":10,\r\n",
    "               \"吊经\":11,\"粗维\":12,\"纬缩\":13,\"浆斑\":14,\r\n",
    "               \"整经结\":15,\"星跳\":16,\"跳花\":16,\"断氨纶\":17,\r\n",
    "               \"稀密档\":18,\"浪纹档\":18,\"色差档\":18,\"磨痕\":19,\r\n",
    "               \"轧痕\":19,\"修痕\":19,\"烧毛痕\":19,\"死皱\":20,\r\n",
    "               \"云织\":20,\"双纬\":20,\"双经\":20,\"跳纱\":20,\r\n",
    "               \"筘路\":20,\"纬纱不良\":20}\r\n",
    "\r\n",
    "id2category = {1:\"破洞\", 2:random.choice([\"水渍\",\"油渍\",\"污渍\"]),\r\n",
    "               3:\"三丝\", 4:\"结头\", 5:\"花板跳\",\r\n",
    "               6:\"百脚\", 7:\"毛粒\", 8:\"粗经\",\r\n",
    "               9:\"松经\", 10:\"断经\",\r\n",
    "               11:\"吊经\", 12:\"粗维\", 13:\"纬缩\", 14:\"浆斑\",\r\n",
    "               15:\"整经结\",16:random.choice([\"星跳\",\"跳花\"]),\r\n",
    "               17:\"断氨纶\",18:random.choice([\"稀密档\",\"浪纹档\",\"色差档\"]),\r\n",
    "               19:random.choice([\"磨痕\",\"轧痕\",\"修痕\",\"烧毛痕\"]),\r\n",
    "               20:random.choice([\"死皱\",\"云织\",\"双纬\",\"双经\",\"跳纱\",\"筘路\",\"纬纱不良\"])}\r\n",
    "img_label = {}\r\n",
    "for info in js:\r\n",
    "    if info[\"name\"] not in img_label:\r\n",
    "        img_label[info[\"name\"]] = []\r\n",
    "    img_label[info[\"name\"]].append(info[\"bbox\"] + [category2id[info[\"defect_name\"]]])\r\n",
    "    \r\n",
    "cls_num = [0]*20\r\n",
    "cls_type = [\"\"] * 20  #[i for i in range(20)]\r\n",
    "for cls in category2id:\r\n",
    "    index = category2id[cls]\r\n",
    "    cls_type[index - 1] += cls + \" \"\r\n",
    "cls_type[19] = cls_type[19][:10]    \r\n",
    "for info in js:\r\n",
    "    idx = category2id[info[\"defect_name\"]] - 1\r\n",
    "    cls_num[idx] += 1\r\n",
    "\r\n",
    "mean_area = [[] for i in range(20)]\r\n",
    "for info in js:\r\n",
    "    cls = info[\"defect_name\"]\r\n",
    "    bbox = info[\"bbox\"]\r\n",
    "    area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\r\n",
    "    mean_area[category2id[cls] - 1].append(area)\r\n",
    "mean_area = [np.mean(area) for area in mean_area]\r\n",
    "mean_area = np.sqrt(mean_area)\r\n",
    "\r\n",
    "pix_num = ((mean_area < 80) * (mean_area / 8) * cls_num) + \\\r\n",
    "((mean_area < 160) * (mean_area > 80)* (mean_area / 16) * cls_num) + \\\r\n",
    "((mean_area < 320) * (mean_area > 160) * (mean_area / 32) * cls_num) + \\\r\n",
    "((mean_area < 640) * (mean_area > 320)  * (mean_area / 64) * cls_num) + \\\r\n",
    "((mean_area > 640) * (mean_area / 128) * cls_num)\r\n",
    "add_num = (np.max(pix_num)/pix_num) * ((np.array(mean_area)<50) + 1)\r\n",
    "add_num = np.round(add_num * 0.8).astype(np.int32)\r\n",
    "\r\n",
    "add_img_label = {}\r\n",
    "target_path = r\"work/train_dat/guangdong1_round1_train1_20190818/defect_Images\"\r\n",
    "src_path = r\"work/train_dat/guangdong1_round1_train1_20190818/defect_Images\"\r\n",
    "norm_path = r\"work/train_dat/guangdong1_round1_train1_20190818/normal_Images\"\r\n",
    "norm_list = os.listdir(norm_path)\r\n",
    "\r\n",
    "test_label = copy.deepcopy(img_label)\r\n",
    "print(\"total = \", len(test_label))\r\n",
    "count = collections.deque(maxlen=300)\r\n",
    "\r\n",
    "for idx, imgname in enumerate(test_label):\r\n",
    "    bbox = np.array(test_label[imgname])\r\n",
    "    bbox = np.round(bbox).astype(np.int32)\r\n",
    "    cls = bbox[:,-1]\r\n",
    "    add_times = []\r\n",
    "    for i in range(cls.shape[0]):\r\n",
    "        add_times.append(add_num[cls[i] - 1])\r\n",
    "    add_times = int(np.mean(add_times))\r\n",
    "    if(add_times == 1):continue\r\n",
    "    img = cv2.imread(os.path.join(src_path, imgname))\r\n",
    "    count.append(add_times)\r\n",
    "    for i in range(1, add_times):\r\n",
    "        new_img_name = imgname[:-4] + \"_\" + str(i) + \".jpg\"\r\n",
    "        new_img = copy.deepcopy(img)\r\n",
    "        new_bbox = copy.deepcopy(bbox)\r\n",
    "        cut_h = random.randint(0,100)\r\n",
    "        cut_w = random.randint(0,200)\r\n",
    "        new_img = new_img[cut_h:, cut_w:]\r\n",
    "        new_bbox[:,[0,2]] = new_bbox[:,[0,2]] - cut_w\r\n",
    "        new_bbox[:,[1,3]] = new_bbox[:,[1,3]] - cut_h\r\n",
    "        if(np.sum(new_bbox < 0) > 0):continue\r\n",
    "        scale = np.array([1000,2446]) / new_img.shape[:2]\r\n",
    "        new_img = cv2.resize(new_img, (2446, 1000))\r\n",
    "        new_bbox[:,[0,2]] = new_bbox[:,[0,2]] * scale[1]\r\n",
    "        new_bbox[:,[1,3]] = new_bbox[:,[1,3]] * scale[0]\r\n",
    "        while(True):\r\n",
    "            norm_add_img = cv2.imread(os.path.join(norm_path, random.choice(norm_list)))\r\n",
    "            if(np.sum(norm_add_img > 200) < 104600):\r\n",
    "                break\r\n",
    "        new_img_mean = np.mean(new_img, axis = (0,1))\r\n",
    "        add_img_mean = np.mean(norm_add_img, axis = (0,1))\r\n",
    "        new_img = (new_img*0.85 + norm_add_img*0.15 * (new_img_mean/add_img_mean) ).astype(np.uint8)\r\n",
    "        add_img_label[new_img_name] = new_bbox\r\n",
    "        cv2.imwrite(os.path.join(target_path, new_img_name), new_img)\r\n",
    "    if(idx  % 200 == 0):\r\n",
    "        print(\"idx = %d, mean_times = %d\"%(idx, np.mean(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final\n"
     ]
    }
   ],
   "source": [
    "print(\"final\")\n",
    "add_js = []\n",
    "for imgname in add_img_label:\n",
    "    anno = add_img_label[imgname].astype(np.float)\n",
    "    for i in range(anno.shape[0]):\n",
    "        temp = {}\n",
    "        temp[\"bbox\"] = list(anno[i])[:4]\n",
    "        temp[\"name\"] = imgname\n",
    "        temp[\"defect_name\"] = id2category[anno[i][-1]]\n",
    "        add_js.append(temp)\n",
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train_add.json\",'w') as fp:\n",
    "    json.dump(add_js, fp, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train_add.json\") as fp:\n",
    "    js1 = json.load(fp)\n",
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train.json\") as fp:\n",
    "    js2 = json.load(fp)\n",
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/anno_train2.json\") as fp:\n",
    "    js3 = json.load(fp)\n",
    "js = js1 + js2 + js3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"work/train_dat/guangdong1_round1_train1_20190818/Annotations/label.json\",'w') as fp:\n",
    "    json.dump(js, fp, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(r\"work/train_dat/guangdong1_round1_train1_20190818/defect_Images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.5.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
